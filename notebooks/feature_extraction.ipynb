{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lundi 18 - 12 - 2023\n",
    "# P6-DS-OC | Classifier automatiquement des biens de consommation\n",
    "\n",
    "\n",
    "<span style=\"color:#FF69B4\" font=10>**TODO**</span>\n",
    "\n",
    "\n",
    "Step 1 - suite : Étude de faisabilité d'un model de catégorisation\n",
    "\n",
    "\n",
    "5. Tester deux approches pour la feature extraction : bag-of-words et Tf-idf\n",
    "6. Test Word2Vec (ou Doc2Vec ou Glove ou FastText)\n",
    "7. Tester Bert avec une approhce word/sentence embedding\n",
    "8. Tester USE\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T08:29:02.757912Z",
     "start_time": "2024-01-14T08:28:57.873557Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CountVectorizer, TfidfVectorizer\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipelines\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_preprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataProcessor\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend \u001B[38;5;28;01mas\u001B[39;00m K\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from src.pipelines.data_preprocessing import DataProcessor\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Extration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_NAME = \"Feature-Extration\"\n",
    "logger = logging.getLogger(LOG_NAME)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data processor class to preprocess the data\n",
    "preprocess = DataProcessor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../data/cleaned_data.csv\", encoding=\"utf-8\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['__len__text']<1300]\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique categories extracted from the specified column.\n",
    "l_cat = list(set(data['category']))\n",
    "\n",
    "# Numerical labels corresponding to the unique categories.\n",
    "y_cat_num = [ (1 - l_cat.index(data.iloc[i]['category'])) for i in range(len(data))]\n",
    "\n",
    "# Create a dictionary to store the results of each model.\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "perplexities = [20, 30, 40]\n",
    "learning_rates = [100, 200]\n",
    "\n",
    "print(\"CountVectorizer : \")\n",
    "print(\"-----------------\")\n",
    "params_ = {}\n",
    "for perplexity in perplexities:\n",
    "    for learning_rate in learning_rates:\n",
    "      ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(X, l_cat, y_cat_num,(2, perplexity, learning_rate,  2000, 42))\n",
    "      params_[ari] = (perplexity, learning_rate)\n",
    "best_parms = params_[max(params_.keys())]\n",
    "logger.info(\"Best parameters for {} : perplexity = {} | learning_rate = {}\".format(str(vectorizer)[:-2], best_parms[0], best_parms[1]))\n",
    "ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(X, l_cat, y_cat_num,(2, best_parms[0], best_parms[1],  2000, 42))\n",
    "preprocess.visualize_tsne(tsne, labels, l_cat, y_cat_num)\n",
    "logger.info(\" {} --> Adjusted Rand Index : {}\".format(str(vectorizer)[:-2], ari))\n",
    "results[str(vectorizer)[:-2]] = max(params_.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "perplexities = [20, 30, 40]\n",
    "learning_rates = [100, 200]\n",
    "\n",
    "print(\"-----------------\")\n",
    "params_ = {}\n",
    "for perplexity in perplexities:\n",
    "    for learning_rate in learning_rates:\n",
    "      ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(X, l_cat, y_cat_num,(2, perplexity, learning_rate,  2000, 42))\n",
    "      params_[ari] = (perplexity, learning_rate)\n",
    "best_parms = params_[max(params_.keys())]\n",
    "logger.info(\"Best parameters for {} : perplexity = {} | learning_rate = {}\".format(str(vectorizer)[:-2], best_parms[0], best_parms[1]))\n",
    "ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(X, l_cat, y_cat_num,(2, best_parms[0], best_parms[1],  2000, 42))\n",
    "preprocess.visualize_tsne(tsne, labels, l_cat, y_cat_num)\n",
    "logger.info(\" {} --> Adjusted Rand Index : {}\".format(str(vectorizer), ari))\n",
    "results[str(vectorizer)] = max(params_.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_size=300\n",
    "w2v_window=5\n",
    "w2v_min_count=1\n",
    "w2v_epochs=100\n",
    "maxlen = 24 # adapt to length of sentences\n",
    "sentences = data['text'].to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création et entraînement du modèle Word2Vec\n",
    "\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window,\n",
    "                                                vector_size=w2v_size,\n",
    "                                                seed=42,\n",
    "                                                workers=1)\n",
    "#                                                workers=multiprocessing.cpu_count())\n",
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des sentences (tokenization)\n",
    "\n",
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "x_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post') \n",
    "                                                   \n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la matrice d'embedding\n",
    "\n",
    "print(\"Create Embedding matrix ...\")\n",
    "w2v_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i=0\n",
    "j=0\n",
    "    \n",
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]\n",
    "            \n",
    "word_rate = np.round(j/i,4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "\n",
    "input=Input(shape=(len(x_sentences),maxlen),dtype='float64')\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')  \n",
    "word_embedding=Embedding(input_dim=vocab_size,\n",
    "                         output_dim=w2v_size,\n",
    "                         weights = [embedding_matrix],\n",
    "                         input_length=maxlen)(word_input)\n",
    "word_vec=GlobalAveragePooling1D()(word_embedding)  \n",
    "embed_model = Model([word_input],word_vec)\n",
    "\n",
    "embed_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_model.predict(x_sentences)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = [20, 30, 40]\n",
    "learning_rates = [100, 200]\n",
    "\n",
    "print(\"-----------------\")\n",
    "params_ = {}\n",
    "for perplexity in perplexities:\n",
    "    for learning_rate in learning_rates:\n",
    "      ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(embeddings, l_cat, y_cat_num,(2, perplexity, learning_rate,  2000, 42))\n",
    "      params_[ari] = (perplexity, learning_rate)\n",
    "best_parms = params_[max(params_.keys())]\n",
    "logger.info(\"Best parameters for {} : perplexity = {} | learning_rate = {}\".format(str(vectorizer)[:-2], best_parms[0], best_parms[1]))\n",
    "ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(embeddings, l_cat, y_cat_num,(2, best_parms[0], best_parms[1],  2000, 42))\n",
    "preprocess.visualize_tsne(tsne, labels, l_cat, y_cat_num)\n",
    "logger.info(\" {} --> Adjusted Rand Index : {}\".format(str(vectorizer), ari))\n",
    "results[str(vectorizer)] = max(params_.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bert\n",
    "# import os\n",
    "# import transformers\n",
    "# from transformers import *\n",
    "\n",
    "# os.environ[\"TF_KERAS\"]='1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
