{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lundi 18 - 12 - 2023\n",
    "# P6-DS-OC | Classifier automatiquement des biens de consommation\n",
    "\n",
    "\n",
    "<span style=\"color:#FF69B4\" font=10>**TODO**</span>\n",
    "\n",
    "\n",
    "Step 1 - suite : Étude de faisabilité d'un model de catégorisation\n",
    "\n",
    "\n",
    "5. Tester deux approches pour la feature extraction : bag-of-words et Tf-idf\n",
    "6. Test Word2Vec (ou Doc2Vec ou Glove ou FastText)\n",
    "7. Tester Bert avec une approhce word/sentence embedding\n",
    "8. Tester USE\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T18:08:31.701057Z",
     "start_time": "2024-01-14T18:08:31.694731Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from src.pipelines.preprocess import DataProcessor\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Extration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T18:08:37.657037Z",
     "start_time": "2024-01-14T18:08:34.327717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"2024-01-14 19:08:34,325 - Data-Preprocessing - INFO - ========== Initializing ==========\"\n",
      "\"2024-01-14 19:08:34,328 - Data-Preprocessing - INFO - Data directory is loaded !!!\"\n",
      "\"2024-01-14 19:08:34,328 - Data-Preprocessing - INFO - Outputs directory is loaded !!!\"\n",
      "\"2024-01-14 19:08:37,647 - Data-Preprocessing - INFO - Spacy model en_core_web_trf is loaded !!!\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data processor class to preprocess the data\n",
    "preprocess = DataProcessor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T18:08:37.793838Z",
     "start_time": "2024-01-14T18:08:37.765557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                            uniq_id  \\\n0  55b85ea15a1536d46b7190ad6fff8ce7   \n1  7b72c92c2f6c40268628ec5f14c6d590   \n2  64d5d4a258243731dc7bbb1eef49ad74   \n3  d4684dcdc759dd9cdf41504698d737d8   \n4  6325b6870c54cd47be6ebfbffa620ec7   \n\n                                         description          category  \\\n0  Key Features of Elegance Polyester Multicolor ...  Home Furnishing    \n1  Specifications of Sathiyas Cotton Bath Towel (...        Baby Care    \n2  Key Features of Eurospa Cotton Terry Face Towe...        Baby Care    \n3  Key Features of SANTOSH ROYAL FASHION Cotton P...  Home Furnishing    \n4  Key Features of Jaipur Print Cotton Floral Kin...  Home Furnishing    \n\n   _len_description                                               text  \\\n0              1420  key elegance polyester multicolor abstract eye...   \n1               444  specification sathiyas cotton bath towel bath ...   \n2              1258  key eurospa cotton terry towel set size small ...   \n3               858  key santosh royal fashion cotton print king si...   \n4              1197  key jaipur print cotton floral king sized doub...   \n\n   _len__text  \n0         998  \n1         371  \n2         891  \n3         689  \n4         919  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uniq_id</th>\n      <th>description</th>\n      <th>category</th>\n      <th>_len_description</th>\n      <th>text</th>\n      <th>_len__text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>55b85ea15a1536d46b7190ad6fff8ce7</td>\n      <td>Key Features of Elegance Polyester Multicolor ...</td>\n      <td>Home Furnishing</td>\n      <td>1420</td>\n      <td>key elegance polyester multicolor abstract eye...</td>\n      <td>998</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b72c92c2f6c40268628ec5f14c6d590</td>\n      <td>Specifications of Sathiyas Cotton Bath Towel (...</td>\n      <td>Baby Care</td>\n      <td>444</td>\n      <td>specification sathiyas cotton bath towel bath ...</td>\n      <td>371</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>64d5d4a258243731dc7bbb1eef49ad74</td>\n      <td>Key Features of Eurospa Cotton Terry Face Towe...</td>\n      <td>Baby Care</td>\n      <td>1258</td>\n      <td>key eurospa cotton terry towel set size small ...</td>\n      <td>891</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d4684dcdc759dd9cdf41504698d737d8</td>\n      <td>Key Features of SANTOSH ROYAL FASHION Cotton P...</td>\n      <td>Home Furnishing</td>\n      <td>858</td>\n      <td>key santosh royal fashion cotton print king si...</td>\n      <td>689</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6325b6870c54cd47be6ebfbffa620ec7</td>\n      <td>Key Features of Jaipur Print Cotton Floral Kin...</td>\n      <td>Home Furnishing</td>\n      <td>1197</td>\n      <td>key jaipur print cotton floral king sized doub...</td>\n      <td>919</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"../outputs/data/data_clean_20240114.csv\", encoding=\"utf-8\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T18:08:39.559953Z",
     "start_time": "2024-01-14T18:08:39.552950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1035 entries, 0 to 1034\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   uniq_id           1035 non-null   object\n",
      " 1   description       1035 non-null   object\n",
      " 2   category          1035 non-null   object\n",
      " 3   _len_description  1035 non-null   int64 \n",
      " 4   text              1035 non-null   object\n",
      " 5   _len__text        1035 non-null   int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 48.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T18:09:31.604750Z",
     "start_time": "2024-01-14T18:09:31.598398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'TfidfVectorizer()'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T16:20:08.441818Z",
     "start_time": "2024-01-14T16:20:07.437957Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'__len__text'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3791\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3790\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3791\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3792\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:181\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: '__len__text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m data \u001B[38;5;241m=\u001B[39m data[\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m__len__text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m1300\u001B[39m]\n\u001B[1;32m      2\u001B[0m data\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/pandas/core/frame.py:3893\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3891\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3892\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3893\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3895\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3798\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3793\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3794\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3795\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3796\u001B[0m     ):\n\u001B[1;32m   3797\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3798\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3799\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3800\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3801\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3802\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3803\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: '__len__text'"
     ]
    }
   ],
   "source": [
    "data = data[data['__len__text']<1300]\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-14T16:20:08.433069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unique categories extracted from the specified column.\n",
    "l_cat = list(set(data['category']))\n",
    "\n",
    "# Numerical labels corresponding to the unique categories.\n",
    "y_cat_num = [ (1 - l_cat.index(data.iloc[i]['category'])) for i in range(len(data))]\n",
    "\n",
    "# Create a dictionary to store the results of each model.\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T16:20:09.813655Z",
     "start_time": "2024-01-14T16:20:09.568320Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m CountVectorizer(stop_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m, max_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.95\u001B[39m, min_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mvectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m perplexities \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m20\u001B[39m, \u001B[38;5;241m30\u001B[39m, \u001B[38;5;241m40\u001B[39m]\n\u001B[1;32m      5\u001B[0m learning_rates \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m200\u001B[39m]\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1150\u001B[0m     )\n\u001B[1;32m   1151\u001B[0m ):\n\u001B[0;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1389\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   1381\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1382\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1383\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1384\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1385\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1386\u001B[0m             )\n\u001B[1;32m   1387\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m-> 1389\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[1;32m   1392\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1276\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[0;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[1;32m   1274\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[1;32m   1275\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m-> 1276\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1277\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1278\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:105\u001B[0m, in \u001B[0;36m_analyze\u001B[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decoder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 105\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m analyzer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    107\u001B[0m     doc \u001B[38;5;241m=\u001B[39m analyzer(doc)\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:238\u001B[0m, in \u001B[0;36m_VectorizerMixin.decode\u001B[0;34m(self, doc)\u001B[0m\n\u001B[1;32m    235\u001B[0m     doc \u001B[38;5;241m=\u001B[39m doc\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode_error)\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m doc \u001B[38;5;129;01mis\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan:\n\u001B[0;32m--> 238\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    239\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    240\u001B[0m     )\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m doc\n",
      "\u001B[0;31mValueError\u001B[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "perplexities = [20, 30, 40]\n",
    "learning_rates = [100, 200]\n",
    "\n",
    "print(\"CountVectorizer : \")\n",
    "print(\"-----------------\")\n",
    "params_ = {}\n",
    "for perplexity in perplexities:\n",
    "    for learning_rate in learning_rates:\n",
    "      ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(X, l_cat, y_cat_num,(2, perplexity, learning_rate,  2000, 42))\n",
    "      params_[ari] = (perplexity, learning_rate)\n",
    "best_parms = params_[max(params_.keys())]\n",
    "logger.info(\"Best parameters for {} : perplexity = {} | learning_rate = {}\".format(str(vectorizer)[:-2], best_parms[0], best_parms[1]))\n",
    "ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(X, l_cat, y_cat_num,(2, best_parms[0], best_parms[1],  2000, 42))\n",
    "preprocess.visualize_tsne(tsne, labels, l_cat, y_cat_num)\n",
    "logger.info(\" {} --> Adjusted Rand Index : {}\".format(str(vectorizer)[:-2], ari))\n",
    "results[str(vectorizer)[:-2]] = max(params_.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T18:08:28.315736Z",
     "start_time": "2024-01-14T18:08:28.237575Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m TfidfVectorizer(stop_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m, max_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.95\u001B[39m, min_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mvectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m perplexities \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m20\u001B[39m, \u001B[38;5;241m30\u001B[39m, \u001B[38;5;241m40\u001B[39m]\n\u001B[1;32m      5\u001B[0m learning_rates \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m200\u001B[39m]\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2139\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   2132\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[1;32m   2133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf \u001B[38;5;241m=\u001B[39m TfidfTransformer(\n\u001B[1;32m   2134\u001B[0m     norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm,\n\u001B[1;32m   2135\u001B[0m     use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_idf,\n\u001B[1;32m   2136\u001B[0m     smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmooth_idf,\n\u001B[1;32m   2137\u001B[0m     sublinear_tf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msublinear_tf,\n\u001B[1;32m   2138\u001B[0m )\n\u001B[0;32m-> 2139\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[1;32m   2141\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[1;32m   2142\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1150\u001B[0m     )\n\u001B[1;32m   1151\u001B[0m ):\n\u001B[0;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1389\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   1381\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1382\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1383\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1384\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1385\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1386\u001B[0m             )\n\u001B[1;32m   1387\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m-> 1389\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[1;32m   1392\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1276\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[0;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[1;32m   1274\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[1;32m   1275\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m-> 1276\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1277\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1278\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:105\u001B[0m, in \u001B[0;36m_analyze\u001B[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decoder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 105\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m analyzer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    107\u001B[0m     doc \u001B[38;5;241m=\u001B[39m analyzer(doc)\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:238\u001B[0m, in \u001B[0;36m_VectorizerMixin.decode\u001B[0;34m(self, doc)\u001B[0m\n\u001B[1;32m    235\u001B[0m     doc \u001B[38;5;241m=\u001B[39m doc\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode_error)\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m doc \u001B[38;5;129;01mis\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan:\n\u001B[0;32m--> 238\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    239\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    240\u001B[0m     )\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m doc\n",
      "\u001B[0;31mValueError\u001B[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=1)\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "perplexities = [20, 30, 40]\n",
    "learning_rates = [100, 200]\n",
    "\n",
    "print(\"-----------------\")\n",
    "params_ = {}\n",
    "for perplexity in perplexities:\n",
    "    for learning_rate in learning_rates:\n",
    "      ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(X, l_cat, y_cat_num,(2, perplexity, learning_rate,  2000, 42))\n",
    "      params_[ari] = (perplexity, learning_rate)\n",
    "best_parms = params_[max(params_.keys())]\n",
    "logger.info(\"Best parameters for {} : perplexity = {} | learning_rate = {}\".format(str(vectorizer)[:-2], best_parms[0], best_parms[1]))\n",
    "ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(X, l_cat, y_cat_num,(2, best_parms[0], best_parms[1],  2000, 42))\n",
    "preprocess.visualize_tsne(tsne, labels, l_cat, y_cat_num)\n",
    "logger.info(\" {} --> Adjusted Rand Index : {}\".format(str(vectorizer), ari))\n",
    "results[str(vectorizer)] = max(params_.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T16:20:13.890934Z",
     "start_time": "2024-01-14T16:20:13.779667Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, float found",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m maxlen \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m24\u001B[39m \u001B[38;5;66;03m# adapt to length of sentences\u001B[39;00m\n\u001B[1;32m      6\u001B[0m sentences \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_list()\n\u001B[0;32m----> 7\u001B[0m sentences \u001B[38;5;241m=\u001B[39m [gensim\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39msimple_preprocess(text) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m sentences]\n",
      "Cell \u001B[0;32mIn[9], line 7\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      5\u001B[0m maxlen \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m24\u001B[39m \u001B[38;5;66;03m# adapt to length of sentences\u001B[39;00m\n\u001B[1;32m      6\u001B[0m sentences \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_list()\n\u001B[0;32m----> 7\u001B[0m sentences \u001B[38;5;241m=\u001B[39m [\u001B[43mgensim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimple_preprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m sentences]\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/gensim/utils.py:311\u001B[0m, in \u001B[0;36msimple_preprocess\u001B[0;34m(doc, deacc, min_len, max_len)\u001B[0m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msimple_preprocess\u001B[39m(doc, deacc\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, min_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, max_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m):\n\u001B[1;32m    289\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\u001B[39;00m\n\u001B[1;32m    290\u001B[0m \n\u001B[1;32m    291\u001B[0m \u001B[38;5;124;03m    Uses :func:`~gensim.utils.tokenize` internally.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    308\u001B[0m \n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    310\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m--> 311\u001B[0m         token \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlower\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeacc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdeacc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mignore\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    312\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m min_len \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(token) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m max_len \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    313\u001B[0m     ]\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokens\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/gensim/utils.py:262\u001B[0m, in \u001B[0;36mtokenize\u001B[0;34m(text, lowercase, deacc, encoding, errors, to_lower, lower)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Iteratively yield tokens as unicode strings, optionally removing accent marks and lowercasing it.\u001B[39;00m\n\u001B[1;32m    229\u001B[0m \n\u001B[1;32m    230\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    259\u001B[0m \n\u001B[1;32m    260\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    261\u001B[0m lowercase \u001B[38;5;241m=\u001B[39m lowercase \u001B[38;5;129;01mor\u001B[39;00m to_lower \u001B[38;5;129;01mor\u001B[39;00m lower\n\u001B[0;32m--> 262\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[43mto_unicode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lowercase:\n\u001B[1;32m    264\u001B[0m     text \u001B[38;5;241m=\u001B[39m text\u001B[38;5;241m.\u001B[39mlower()\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/gensim/utils.py:365\u001B[0m, in \u001B[0;36many2unicode\u001B[0;34m(text, encoding, errors)\u001B[0m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    364\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n\u001B[0;32m--> 365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: decoding to str: need a bytes-like object, float found"
     ]
    }
   ],
   "source": [
    "w2v_size=300\n",
    "w2v_window=5\n",
    "w2v_min_count=1\n",
    "w2v_epochs=100\n",
    "maxlen = 24 # adapt to length of sentences\n",
    "sentences = data['text'].to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T16:20:28.723196Z",
     "start_time": "2024-01-14T16:20:28.610980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build & train Word2Vec model ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 9\u001B[0m\n\u001B[1;32m      4\u001B[0m w2v_model \u001B[38;5;241m=\u001B[39m gensim\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mWord2Vec(min_count\u001B[38;5;241m=\u001B[39mw2v_min_count, window\u001B[38;5;241m=\u001B[39mw2v_window,\n\u001B[1;32m      5\u001B[0m                                                 vector_size\u001B[38;5;241m=\u001B[39mw2v_size,\n\u001B[1;32m      6\u001B[0m                                                 seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m,\n\u001B[1;32m      7\u001B[0m                                                 workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m#                                                workers=multiprocessing.cpu_count())\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[43mw2v_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m w2v_model\u001B[38;5;241m.\u001B[39mtrain(sentences, total_examples\u001B[38;5;241m=\u001B[39mw2v_model\u001B[38;5;241m.\u001B[39mcorpus_count, epochs\u001B[38;5;241m=\u001B[39mw2v_epochs)\n\u001B[1;32m     11\u001B[0m model_vectors \u001B[38;5;241m=\u001B[39m w2v_model\u001B[38;5;241m.\u001B[39mwv\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/gensim/models/word2vec.py:491\u001B[0m, in \u001B[0;36mWord2Vec.build_vocab\u001B[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001B[0m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001B[39;00m\n\u001B[1;32m    454\u001B[0m \n\u001B[1;32m    455\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    488\u001B[0m \n\u001B[1;32m    489\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    490\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, passes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 491\u001B[0m total_words, corpus_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscan_vocab\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress_per\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_per\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrim_rule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count \u001B[38;5;241m=\u001B[39m corpus_count\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_total_words \u001B[38;5;241m=\u001B[39m total_words\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/gensim/models/word2vec.py:586\u001B[0m, in \u001B[0;36mWord2Vec.scan_vocab\u001B[0;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001B[0m\n\u001B[1;32m    583\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_file:\n\u001B[1;32m    584\u001B[0m     corpus_iterable \u001B[38;5;241m=\u001B[39m LineSentence(corpus_file)\n\u001B[0;32m--> 586\u001B[0m total_words, corpus_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_scan_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress_per\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    588\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m    589\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcollected \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m word types from a corpus of \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m raw words and \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m sentences\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    590\u001B[0m     \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw_vocab), total_words, corpus_count\n\u001B[1;32m    591\u001B[0m )\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_words, corpus_count\n",
      "File \u001B[0;32m~/git/p6-ds/.venv/lib/python3.10/site-packages/gensim/models/word2vec.py:569\u001B[0m, in \u001B[0;36mWord2Vec._scan_vocab\u001B[0;34m(self, sentences, progress_per, trim_rule)\u001B[0m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sentence_no \u001B[38;5;241m%\u001B[39m progress_per \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    565\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m    566\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPROGRESS: at sentence #\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m, processed \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m words, keeping \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m word types\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    567\u001B[0m         sentence_no, total_words, \u001B[38;5;28mlen\u001B[39m(vocab),\n\u001B[1;32m    568\u001B[0m     )\n\u001B[0;32m--> 569\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m sentence:\n\u001B[1;32m    570\u001B[0m     vocab[word] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    571\u001B[0m total_words \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(sentence)\n",
      "\u001B[0;31mTypeError\u001B[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Création et entraînement du modèle Word2Vec\n",
    "\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window,\n",
    "                                                vector_size=w2v_size,\n",
    "                                                seed=42,\n",
    "                                                workers=1)\n",
    "#                                                workers=multiprocessing.cpu_count())\n",
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des sentences (tokenization)\n",
    "\n",
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "x_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post') \n",
    "                                                   \n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la matrice d'embedding\n",
    "\n",
    "print(\"Create Embedding matrix ...\")\n",
    "w2v_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i=0\n",
    "j=0\n",
    "    \n",
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]\n",
    "            \n",
    "word_rate = np.round(j/i,4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "\n",
    "input=Input(shape=(len(x_sentences),maxlen),dtype='float64')\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')  \n",
    "word_embedding=Embedding(input_dim=vocab_size,\n",
    "                         output_dim=w2v_size,\n",
    "                         weights = [embedding_matrix],\n",
    "                         input_length=maxlen)(word_input)\n",
    "word_vec=GlobalAveragePooling1D()(word_embedding)  \n",
    "embed_model = Model([word_input],word_vec)\n",
    "\n",
    "embed_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_model.predict(x_sentences)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = [20, 30, 40]\n",
    "learning_rates = [100, 200]\n",
    "\n",
    "print(\"-----------------\")\n",
    "params_ = {}\n",
    "for perplexity in perplexities:\n",
    "    for learning_rate in learning_rates:\n",
    "      ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(embeddings, l_cat, y_cat_num,(2, perplexity, learning_rate,  2000, 42))\n",
    "      params_[ari] = (perplexity, learning_rate)\n",
    "best_parms = params_[max(params_.keys())]\n",
    "logger.info(\"Best parameters for {} : perplexity = {} | learning_rate = {}\".format(str(vectorizer)[:-2], best_parms[0], best_parms[1]))\n",
    "ari, tsne, labels = preprocess.perform_tsne_ari_kmeans(embeddings, l_cat, y_cat_num,(2, best_parms[0], best_parms[1],  2000, 42))\n",
    "preprocess.visualize_tsne(tsne, labels, l_cat, y_cat_num)\n",
    "logger.info(\" {} --> Adjusted Rand Index : {}\".format(str(vectorizer), ari))\n",
    "results[str(vectorizer)] = max(params_.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bert\n",
    "# import os\n",
    "# import transformers\n",
    "# from transformers import *\n",
    "\n",
    "# os.environ[\"TF_KERAS\"]='1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
